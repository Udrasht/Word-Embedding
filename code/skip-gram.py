# -*- coding: utf-8 -*-
"""skipgram2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MnCks7crwbhTxLgqKVU4jhqfFl_UWoCZ
"""

import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from torchtext.data import get_tokenizer
import string
import pandas as pd
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import matplotlib.pyplot as plt
from collections import deque
from nltk.stem import WordNetLemmatizer
import torch.nn.functional as F
import nltk
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import json

data = pd.read_csv("../data/train.csv", nrows=30000)
corpus = data['Description']
stop_words = set(stopwords.words('english'))
sentences=[]
for sent in corpus:
    sentences.append(sent)

tokenizer=get_tokenizer("basic_english")





class DataPreprocessing:
    def __init__(self, input_file):
        self.input_file = input_file
        self.vocab = None
        self.word_freq = dict()
        self.word2id = dict()
        self.id2word = dict()
        self.word_pairs = deque()
        self.prepared_documents = []
        self.document_count = 1
        self.sample_table = []
        self.doc_amount = 0
        self.tocken=False
        self.build_vocab()
        self.make_sample_table()
    def preprocess(self,sentence):
      text = sentence
      text = text.lower()
      text_p = "".join([char for char in text if char not in string.punctuation])
      text = re.sub(r'\b\d+\b', '', text)
      if text_p not in stop_words:
        return text_p
      else:
        return ""


    def build_vocab(self):


        word_freq = dict()

        lemma = WordNetLemmatizer()
        w_id = 0
        w = []
        prep_documents = []
        processed_sentences=[]
        count=0

        for sentence in self.input_file:
          sent = tokenizer(sentence)

          tocken_sentense=[]

          for s in sent:

            s = self.preprocess(s)

            if len(s)!=0:
              tocken_sentense.append(s)
          processed_sentences.append(tocken_sentense)
        for words in processed_sentences:

            w = []
            for word in words:
                # print(word)
              if word not in word_freq:
                word_freq[word] = 1
              else:
                word_freq[word] += 1
              w.append(word)

            prep_documents.append(w)
            self.doc_amount += 1

        # print("dhdh")
        max_freq = max(word_freq.values())
        high_bound = max_freq
        w_id = 1
        self.word2id['UNK']=0
        self.id2word[0] = 'UNK'
        for word, freq in word_freq.items():
            if freq > 5 and freq <= high_bound:
                self.word2id[word] = w_id
                self.id2word[w_id] = word
                self.word_freq[w_id] = freq
                w_id+=1


        # self.prepared_documents = [[word for word in document if word in self.word2id] for document in prep_documents]
        for document in prep_documents:
          filtered_document = []
          for word in document:
              if word in self.word2id:
                  filtered_document.append(word)
              else:
                filtered_document.append('UNK')
          self.prepared_documents.append(filtered_document)

        self.vocab = list(self.word2id.keys())


    def get_batch_pairs(self, batch_size, context_size):

        while self.document_count%5000 != 0:
            if self.document_count>= self.doc_amount:
              self.document_count=1
              self.tocken=True
              break
            doc_num = self.document_count % self.doc_amount
            doc_words_ids = []

            for word in self.prepared_documents[doc_num]:
                doc_words_ids.append(self.word2id[word])
            self.document_count+=1
            for ind_center, center_word_id in enumerate(doc_words_ids):
              for ind_context, context_word_id in enumerate(doc_words_ids[max(ind_center - context_size, 0):ind_center+context_size+1]):
                  if ind_center != ind_context:
                      self.word_pairs.append((center_word_id, context_word_id))
        batch_pairs = []
        # print("pal",self.document_count)
        n1 = len(self.word_pairs)
        self.document_count+=1
        # print(self.document_count)
        # print(n1)
        batch = 64

        if n1 % batch != 0:
            n1 -= (n1 % batch)
        # print(n1)
        # print()
        for _ in range(n1):
            batch_pairs.append(self.word_pairs.popleft())
        self.word_pairs.clear()

        return batch_pairs
    def alter_token(self):
        self.tocken=False

    def get_negative_pairs(self, pos_words_pair, k):

        neg_pairs = np.random.choice(self.sample_table, size=(k, len(pos_words_pair))).tolist()
        return neg_pairs

    def make_sample_table(self):

        numerator = np.array(list(self.word_freq.values())) ** 0.75
        table_size = 1e8
        denominator = sum(numerator)
        ratio = numerator / denominator
        count = np.round(ratio * table_size)
        for word_ind, count in enumerate(count):
            self.sample_table += [word_ind] * int(count)

        self.sample_table = np.array(self.sample_table)

class Skipgram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(Skipgram, self).__init__()
        self.embedding_dim = embedding_dim
        self.u_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)
        print(self.u_embeddings)
        self.v_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)
        print(self.v_embeddings)
        initrange = 0.5 / self.embedding_dim
        self.u_embeddings.weight.data.uniform_(-initrange, initrange)
        self.v_embeddings.weight.data.uniform_(-0,0)


    def forward(self, input_words, context_words, neg_words):

        emb_u = self.u_embeddings(input_words)
        emb_v = self.v_embeddings(context_words)
        negative_value = 0
        positive_value = F.logsigmoid(torch.sum(torch.diag(torch.mm(emb_u, emb_v.t()))))

        for neg in neg_words:
            neg_emb = self.v_embeddings(neg)
            neg_value = torch.sum(torch.diag(torch.mm(emb_u, neg_emb.t())))
            negative_value += F.logsigmoid(-neg_value)

        loss = -1 * (positive_value + negative_value)

        return loss


    def get_embedding(self, word):
        embedding = self.u_embeddings.weight.data.numpy()

        return embedding[word]

class Word2vec:
    def __init__(self,
                 batch_size = 64,
                 context_size = 5,
                 embedding_dim = 300,
                 epoch_num = 10,
                 num_neg_words = 3,
                 lr = 0.001):
        super(Word2vec, self).__init__()

        self.batch_size = batch_size
        self.context_size = context_size
        self.num_neg_words = num_neg_words
        self.lr = lr
        self.embedding_dim = embedding_dim
        self.epoch_num = epoch_num


        self.use_cuda = torch.cuda.is_available()
        if self.use_cuda:
            self.model.cuda()


    def fit(self, sentences, use_pretrained_model = False, file_with_model = None, draw_losses = True):

        self.prep_data = DataPreprocessing(sentences)

        self.vocab_size = len(self.prep_data.vocab)


        self.model = Skipgram(self.vocab_size, self.embedding_dim)

        if use_pretrained_model:
            self.load_model(file_with_model)

        self.optimizer = optim.SGD(self.model.parameters(), lr = self.lr)
        losses = []

        for epoch in tqdm(range(self.epoch_num)):

            while self.prep_data.tocken !=True:
              positive_pairs = self.prep_data.get_batch_pairs(self.batch_size, self.context_size)

              batched_positive_pairs = [positive_pairs[i:i + self.batch_size] for i in range(0, len(positive_pairs), self.batch_size)]

              for positive_batch in batched_positive_pairs:
                  pos_centers = torch.LongTensor([pair[0] for pair in positive_batch])
                  pos_contexts = torch.LongTensor([pair[1] for pair in positive_batch])

                  # Get negative neighbors
                  negative_neighbors = self.prep_data.get_negative_pairs(positive_batch, self.num_neg_words)
                  negative_neighbors = Variable(torch.LongTensor(negative_neighbors))

                  # Move tensors to GPU if necessary
                  if self.use_cuda:
                      pos_centers = pos_centers.cuda()
                      pos_contexts = pos_contexts.cuda()
                      negative_neighbors = negative_neighbors.cuda()

                  # Zero gradients, forward pass, backward pass, and optimization
                  self.optimizer.zero_grad()
                  loss = self.model.forward(pos_centers, pos_contexts, negative_neighbors)
                  loss.backward()
                  self.optimizer.step()



            losses.append(loss.data)
            print(loss.data,"loss")

            self.prep_data.alter_token()

        self.save_embeddings('skip_gram_embedding')



    def get_similar(self, target_word, n_similar):
        target_word_emb = self.get_embedding(target_word)
        norm_target_word = np.linalg.norm(target_word_emb)
        values = []
        for word in self.prep_data.vocab:
            if word != target_word:
                word_emb = self.get_embedding(word)
                norm_word = np.linalg.norm(word_emb)
                dot_product = np.dot(target_word_emb, word_emb)
                value = dot_product / (norm_target_word * norm_word)
                values.append((word, value))

        return [(w,v) for w, v in sorted(values, key = lambda x: x[1], reverse = True)][:n_similar]


    def save_model(self, output_file):
        torch.save(self.model.state_dict(), output_file)

    def load_model(self, input_file):
        self.model.load_state_dict(torch.load(input_file))

    def get_embedding(self, word):
        word_ind = self.prep_data.word2id[word]
        return self.model.get_embedding(word_ind)


    def save_embeddings(self, output_file):
      embeddings = self.model.u_embeddings.weight.data.numpy()
      embeddings_dict = {}
      for word_id, word in self.prep_data.id2word.items():
          emb = embeddings[word_id]
          embeddings_dict[word] = emb.tolist()

      with open(output_file, 'w') as f:
          json.dump(embeddings_dict, f)

nltk.download('wordnet')
we1=Word2vec()

# we1.fit(sentences)
we1.load_pretrained_model('../models/skip-gram-word-vectors.pt')

# we1.save_model("skip-gram-word-vectors.pt")

def get_embedding( word):
        word_ind = we1.prep_data.word2id[word]
        return we1.model.get_embedding(word_ind)

def get_similar(target_word, n_similar):
        target_word_emb = get_embedding(target_word)
        norm_target_word = np.linalg.norm(target_word_emb)
        values = []
        # values.append((target_word,target_word_emb))
        for word in we1.prep_data.vocab:
            # if word != target_word:
            word_emb = get_embedding(word)
            norm_word = np.linalg.norm(word_emb)
            dot_product = np.dot(target_word_emb, word_emb)
            value = dot_product / (norm_target_word * norm_word)
            values.append((word, value))

        return [(w,v) for w, v in sorted(values, key = lambda x: x[1], reverse = True)][:n_similar]

print(get_similar('run', 10))