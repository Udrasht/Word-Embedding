# -*- coding: utf-8 -*-
"""downstream.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hSU_19xDkyAO250pV7zysXLRT7x2DfhS
"""

from collections import Counter
from torch.utils.data import DataLoader, TensorDataset
import torch
import json
from sklearn.decomposition import IncrementalPCA, PCA
from numpy.linalg import norm
from scipy.sparse.linalg import svds
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import string
from torchtext.data import get_tokenizer
import html
import numpy as np
from tqdm import tqdm
from nltk.tokenize import sent_tokenize, word_tokenize
import nltk
import re
import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import classification_report
import torch.nn as nn
import torch.optim as optim
nltk.download('punkt')
nltk.download('stopwords')



print("Downstream using skip gram context_windo size 5")

data = pd.read_csv("../data/train.csv", nrows=30000)
corpus = data['Description']
Y_train = data['Class Index']

data = pd.read_csv("../data/test.csv")
corpus_test = data['Description']
Y_test = data['Class Index']

embeddings = {}
file_path = "../data/skipgram_embedding"
with open(file_path, 'r') as f:
    embeddings = json.load(f)


def preprocess(sentence):
    text = sentence
    text = text.lower()
    text_p = "".join([char for char in text if char not in string.punctuation])
    text = re.sub(r'\b\d+\b', '', text)
    if text_p not in stop_words:
        return text_p
    else:
        return ""


tokenizer = get_tokenizer("basic_english")


def pre_pros(corpus_temp):
    sentences = []

    for sent in corpus_temp:
        sentences.append(sent)
    # print(len(sentences))
    processed_sentences_temp = []
    count = 0

    for sentence in sentences:
        sent = tokenizer(sentence)

        tocken_sentense = []

        for s in sent:

            s = preprocess(s)

            if len(s) != 0:
                tocken_sentense.append(s)
        processed_sentences_temp.append(tocken_sentense)
    return processed_sentences_temp

# print((embeddings['reuters']))


def add_padding(processed_sentences, max_sequence_len, Y):

    padded_sentences_temp = []
    Y_temp = []
    for sentence, y in zip(processed_sentences, Y):
        if len(sentence) < 5:
            continue
        if len(sentence) > max_sequence_len:
            padded_sentence = sentence[:max_sequence_len]

        else:

            num_pads = max_sequence_len - len(sentence)

            padded_sentence = sentence + ['<PAD>'] * num_pads

        padded_sentences_temp.append(padded_sentence)
        Y_temp.append(y)
    return padded_sentences_temp, Y_temp


def sentence_to_embeddings(sentences, embeddings):
    array = []
    for sent in sentences:
        temp = []

        for word in sent:
            if word == '<PAD>':
                temp.append(PAD_embedding)
            elif word not in embeddings:
                temp.append(UNK_embedding)
            else:

                temp.append(np.array(embeddings[word], dtype=np.float32))
        array.append(temp)
    return np.array(array, dtype=np.float32)


input_size = 300
hidden_size = 128
num_layers = 1
num_classes = 4
learning_rate = 0.001
num_epochs = 10


class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size,
                            num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(
            0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(
            0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out


def train_model(model, train_loader, criterion, optimizer, num_epochs):
    losses = []

    for epoch in tqdm(range(num_epochs)):
        total_loss = 0.0
        correct_train = 0
        total_train = 0
        true_labels_train = []
        predicted_labels_train = []

        # Iterate over batches in the training DataLoader
        for inputs, labels in train_loader:
            # Forward pass
            outputs = model(inputs)
            one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=4)
            one_hot_labels = one_hot_labels.float()

            # Compute loss
            loss = criterion(outputs, one_hot_labels)

            # Calculate training accuracy
            _, predicted = torch.max(outputs, 1)
            total_train += len(labels)
            correct_train += (predicted == labels).sum().item()
            true_labels_train.extend(labels.numpy())
            predicted_labels_train.extend(predicted.numpy())

            # Add the loss to the total loss for this epoch
            total_loss += loss.item()

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # Calculate average loss for this epoch
        epoch_loss = total_loss / len(train_loader)

        # Calculate training accuracy for this epoch
        train_accuracy = correct_train / total_train

        # Append the average loss to the losses list
        losses.append(epoch_loss)

        # Print the average loss and training accuracy for this epoch
        print(
            f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')

    return model, losses, true_labels_train, predicted_labels_train


def test_model(model, test_loader, criterion):
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    true_labels = []
    predicted_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            test_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            total += 1
            correct += (predicted == labels).sum().item()
            true_labels.extend(labels.numpy())
            predicted_labels.extend(predicted.numpy())

    # Calculate average test loss
    test_loss /= len(test_loader.dataset)
    print(f'Test Loss: {test_loss:.4f}')

    # Calculate test accuracy
    test_accuracy = 100 * correct / total
    print(f'Test Accuracy: {test_accuracy:.2f}%')

    return true_labels, predicted_labels


def plot_confusion_matrix_and_classification_report(true_labels, predicted_labels, class_names, test_train):
    # Compute the confusion matrix
    conf_matrix = confusion_matrix(true_labels, predicted_labels)

    # Plot the confusion matrix
    plt.figure(figsize=(10, 8))
    sns.set(font_scale=1.2)  # Adjust font scale
    sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix'+test_train)
    plt.show()

    # Plot the classification report

    report = classification_report(
        true_labels, predicted_labels, target_names=class_names, output_dict=True)
    df = pd.DataFrame(report).transpose()
    sns.heatmap(df.iloc[:-1, :].T, annot=True, cmap="YlGnBu", cbar=False)
    plt.title('Classification Report'+test_train)
    plt.xlabel('Metrics')
    plt.ylabel('Classes')
    plt.show()


def pre_pros_input(corpus_temp):
    sentences = input_sent
    sent = tokenizer(sentences)
    processed_sentences_temp = []

    tocken_sentense = []

    for s in sent:

        s = preprocess(s)

        if len(s) != 0:
            tocken_sentense.append(s)
    processed_sentences_temp.append(tocken_sentense)

    return processed_sentences_temp


def find_lable_of_user_input(input_sent, labels):
    procesed_input = pre_pros_input(input_sent)
    padded_sentences_input, Y_input = add_padding(procesed_input, 35, [[1]])
    input_embeddings = sentence_to_embeddings(
        padded_sentences_input, embeddings)
    user_input = np.array(input_embeddings)
    user_input_tensor = torch.tensor(user_input)
    model.eval()
    predicted_input = []
    with torch.no_grad():
        outputs = model(user_input_tensor)
        # loss = criterion(outputs, labels)

        _, predicted = torch.max(outputs, 1)
        predicted_input.extend(predicted.numpy())

    # Calculate average test loss
    for i in predicted_input:
        print("class", i+1)


print("vocab size", len(embeddings))

label_counts = Counter(Y_train)

for label, count in label_counts.items():
    print(f"Label {label}: {count} occurrences")

stop_words = set(stopwords.words('english'))

processed_sentences = pre_pros(corpus)
processed_sentences_test = pre_pros(corpus_test)


sentence_lengths = [len(sentence) for sentence in processed_sentences]
# print(sentence_lengths)


# plt.figure(figsize=(10, 6))
# plt.hist(sentence_lengths, bins=50, color='red', edgecolor='black')
# plt.title('Distribution of Sentence Lengths')
# plt.xlabel('Length of Sentence')
# plt.ylabel('Frequency')
# plt.grid(True)
# plt.show()

# print(len(processed_sentences))
# print(len(processed_sentences_test))


padded_sentences, Y_train = add_padding(processed_sentences, 35, Y_train)
padded_sentences_test, Y_test = add_padding(
    processed_sentences_test, 35, Y_test)

# print(len(padded_sentences))
# print(len(padded_sentences_test))

PAD_embedding = np.zeros(300, dtype=np.float32)
UNK_embedding = np.array(embeddings['UNK'], dtype=np.float32)


X_train_embeddings = sentence_to_embeddings(padded_sentences, embeddings)
X_test_embeddings = sentence_to_embeddings(padded_sentences_test, embeddings)

print("Number of train sentence",X_train_embeddings.shape)
print("Number of test sentence",X_test_embeddings.shape)

y_train_tensor = torch.tensor(Y_train) - 1
y_test_tensor = torch.tensor(Y_test) - 1

# Create a TensorDataset with variable-length sequences
# print("bye")
train_data = TensorDataset(torch.tensor(X_train_embeddings), y_train_tensor)

# # Create DataLoader
batch_size = 64
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

test_data = TensorDataset(torch.tensor(X_test_embeddings), y_test_tensor)
test_loader = DataLoader(test_data, batch_size=1, shuffle=False)


'''for train the model un comment the below lines'''
# model = LSTM(input_size, hidden_size, num_layers, num_classes)
# criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(model.parameters(), lr=learning_rate)


# model, train_losses, true_labels_train, predicted_labels_train = train_model(model, train_loader, criterion, optimizer, num_epochs)

# plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.title('Training Loss vs Epoch')
# plt.legend()
# plt.show()

# torch.save(model.state_dict(), 'trained_model.pt')

# Load the pre-trained model for testing
model = LSTM(input_size, hidden_size, num_layers, num_classes)
criterion = nn.CrossEntropyLoss()
model.load_state_dict(torch.load('../models/skip-gram-classification-model.pt'))

# # Set the model to evaluation mode
# pretrained_model.eval()


# Assuming you have already defined test_loader, criterion, etc.

# Load the pre-trained model for testing
# pretrained_model = LSTM(input_size, hidden_size, num_layers, num_classes)
# pretrained_model.load_state_dict(torch.load('trained_model.pt'))

# Test the pre-trained model
true_labels, predicted_labels = test_model(model, test_loader, criterion)

true_labels_test = np.array(true_labels)
predicted_labels_test = np.array(predicted_labels)

# true_labels_train = np.array(true_labels_train)
# predicted_labels_train = np.array(predicted_labels_train)

class_names = ['Class 1', 'Class 2', 'Class 3', 'Class 4']


# plot_confusion_matrix_and_classification_report(true_labels_train, predicted_labels_train, class_names,"Train")
plot_confusion_matrix_and_classification_report(
    true_labels_test, predicted_labels_test, class_names, "Test")


input_sent = "By the way, the good news is you might have just saved another life."
find_lable_of_user_input(input_sent, model)

# print("Model Architecture:")
# print(model)

# Print the model parameters and their shapes
# print("\nModel Parameters:")
# for name, param in model.named_parameters():
#     print(f"Parameter name: {name}, Shape: {param.shape}")

# torch.save(model.state_dict(), 'skip-gram-classification-model.pt')
