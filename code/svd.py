# -*- coding: utf-8 -*-
"""svd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/194qnm5idgtq3OfrYTARr6eRLgmDtEG7Z
"""

import json
import torch
from sklearn.decomposition import IncrementalPCA, PCA
from numpy.linalg import norm
from scipy.sparse.linalg import svds
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import string
from torchtext.data import get_tokenizer
import html
import numpy as np
import pandas as pd
from tqdm import tqdm
from nltk.tokenize import sent_tokenize, word_tokenize
import nltk
import re
nltk.download('punkt')
nltk.download('stopwords')

tokens = ['superb', 'ship', 'run', 'films', 'definitely', 'dance', 'gun']
colors = ['g', 'r', 'b', 'r', 'g', 'y', 'b']


data = pd.read_csv("../data/train.csv", nrows=30000)
corpus = data['Description']

stop_words = set(stopwords.words('english'))


def preprocess(sentence):
    text = sentence
    text = text.lower()
    text_p = "".join([char for char in text if char not in string.punctuation])
    text = re.sub(r'\b\d+\b', '', text)
    if text_p not in stop_words:
        return text_p
    else:
        return ""


class SVDEmbedding:
    def __init__(self, corpus, thr=1, wnd_size=1):
        self.corpus = corpus
        self.thr = thr
        self.vocab = None
        self.wnd_size = wnd_size
        self.vocab_dict = {'UNK': 0}
        self.co_matrix = None
        self.id2word = {}

    def build_vocab(self):
        vocab_dict = {}
        for sentence in self.corpus:
            for word in sentence:

                if word not in vocab_dict:
                    vocab_dict[word] = 1
                else:
                    vocab_dict[word] = vocab_dict[word]+1

        for key, value in vocab_dict.items():
            if value < self.thr:
                self.vocab_dict['UNK'] = self.vocab_dict['UNK']+1
            else:
                self.vocab_dict[key] = value

        for sentence in self.corpus:
            for i in range(len(sentence)):
                if sentence[i] not in self.vocab_dict:
                    sentence[i] = 'UNK'
        id = 0
        for word, count in self.vocab_dict.items():
            self.id2word[id] = word
            id += 1
        # print(self.corpus[0])
        self.vocab = list(self.vocab_dict.keys())


    def build_SVD(self):
        U, Sigma, VT = svds(self.co_matrix, k=300)
        return U

    def build_co_occurrence_matrix(self):
        matrix = np.zeros((len(self.vocab_dict), len(self.vocab_dict)))
        # self.vocab = list(self.vocab_dict.keys())
        for sentence in self.corpus:
            for i in range(len(sentence)):
                start = max(0, i-self.wnd_size)
                end = min(len(sentence), i+self.wnd_size+1)
                i_word_indx = self.vocab.index(sentence[i])
                for k in range(start, end):
                    if i==k:
                        continue
                    k_word_indx = self.vocab.index(sentence[k])
                    matrix[i_word_indx, k_word_indx] += 1
                    matrix[k_word_indx, i_word_indx] += 1
        self.co_matrix = matrix

    


def save_embeddings(U, output_file):
    embedding_dict = {}
    for word_id, word in we1.id2word.items():
        embedding_dict[word] = U[word_id].tolist()

    with open(output_file, 'w') as f:
        json.dump(embedding_dict, f)


def cosine_dist(u, v):
    return 1-np.dot(u, v)/(norm(u)*norm(v))


def giveTopTenWords(word, df):

    word_indx = we1.vocab.index(word)
    dist = []
    for i in df.index:
        i_word_indx = we1.vocab.index(i)
        dist.append(cosine_dist(
            df.iloc[word_indx, :], df.iloc[i_word_indx, :]))
    near = np.argsort(dist)
    ans = []
    for i in range(11):
        ans.append(we1.vocab[near[i]])
    return ans


def display_pca_scatterplot(model, words, colors):
    pca_fun = PCA(2)
    w_v = []
    for i in words:
        w_v.append(model[i])
    w_v=np.array(w_v)
    temp= pca_fun.fit_transform(w_v)
    plt.figure(figsize=(15, 15))
    for word, (x, y), color in zip(words, temp, colors):
        plt.scatter(x, y, c=color, label=word)
        plt.text(x+0.004, y+0.0005, word)
    plt.legend()
    plt.savefig("word_distribution.png")
    plt.show()


tokenizer = get_tokenizer("basic_english")
sentences = []

for sent in corpus:
    sentences.append(sent)
print("corpus size",len(sentences))
processed_sentences = []
count = 0

for sentence in sentences:
    sent = tokenizer(sentence)
    tocken_sentense = []
    for s in sent:
        s = preprocess(s)
        if len(s) != 0:
            tocken_sentense.append(s)
    processed_sentences.append(tocken_sentense)



we1 = SVDEmbedding(processed_sentences, thr=5, wnd_size=5)
we1.build_vocab()


# we1.build_co_occurrence_matrix()
# U = we1.build_SVD()


U = torch.load('../models/svd-word-vectors.pt')

print("vocab Size:", len(we1.vocab_dict))


df = pd.DataFrame(U, index=we1.vocab_dict)


for j in range(len(tokens)):
    model = {}
    words = giveTopTenWords(tokens[j], df)
    for i in words:
        model[i] = list(df.loc[i])
    display_pca_scatterplot(model, words, [colors[j]] * len(words))


# save_embeddings(U,"SVD_1_30000")

# torch.save(U, 'svd-word-vectors.pt')
